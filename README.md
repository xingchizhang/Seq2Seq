# Seq2Seq

# Introduce
There are several seq2seq models implemented in PyTorch. By exploring these models, you can enhance your understanding of NLP and related research papers.  
This repository utilized the code available at [pytorch_seq2seq](https://github.com/bentrevett/pytorch-seq2seq). A special thanks to Ben Trevett for his tutorials!


# Environment
Python: 3.8  
torch == 1.8.1  
torchtext == 0.9.1  
&emsp;# Ensure that the torchtext version is 0.9; otherwise, an error will occur in init_data.    
&emsp;# Meanwhile, torch is available only in version 1.8 because torch and torchtext have version relationships of a.b.c and a.(b+1).c.     
spacy == 3.7.2  
en_core_web_sm and de_core_news_sm can be found in library, you can use "pip setup.py install" to install these.  

# Correlative Paper
* **Paper Name:**&emsp;Attention is All You Need  
**Accept:**&emsp;Neural Information Processing Systems (NIPS)   
**Time:**&emsp;2017-06-12  
**Paper Reference:**&emsp;https://arxiv.org/pdf/1706.03762.pdf 

* **Paper Name:**&emsp;Convolutional Sequence to Sequence Learning  
**Accept:**&emsp;International conference on machine learning (ICML)  
**Time:**&emsp;2017-05-08  
**Paper Reference:**&emsp;https://arxiv.org/pdf/1705.03122.pdf  

* **Paper Name:**&emsp;Neural Machine Translation by Jointly Learning to Align and Translate  
**Accept:**&emsp;arXiv preprint arXiv:1409.0473  
**Time:**&emsp;2014-09-01  
**Paper Reference:**&emsp;https://arxiv.org/pdf/1409.0473.pdf  

* **Paper Name:**&emsp;Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation  
**Accept:**&emsp;arXiv preprint arXiv:1406.1078  
**Time:**&emsp;2014-06-03  
**Paper Reference:**&emsp;https://arxiv.org/pdf/1406.1078.pdf  

* **Paper Name:**&emsp;Sequence to Sequence Learning with Neural Networks  
**Accept:**&emsp;Neural Information Processing Systems (NIPS)  
**Time:**&emsp;2014-09-10  
**Paper Reference:**&emsp;https://arxiv.org/pdf/1409.3215.pdf  
